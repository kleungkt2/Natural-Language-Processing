{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model\n",
    "\n",
    "Text preprocessing piplines:\n",
    "\n",
    "Text denoising -> text normalization (Tutorial 1) -> text standarization (Tutorial 2)\n",
    "\n",
    "In this tutorial we'll learn how to conduct text standarization using vector space model.\n",
    "\n",
    "1. Stop words, ngrams, the whole pipeline of text preprocessing.\n",
    "2. Bag-of-word representation\n",
    "3. Term weighting\n",
    " - Term frequency\n",
    " - Inverse document frequency\n",
    " - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stop words, ngrams, the whole pipeline of text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tianqing/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords's intuition: Not all words are informative\n",
    "- Remove such words to reduce vocabulary size\n",
    "- No universal definition\n",
    "- Risk: break the original meaning and structure of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'be', 'this', 'as', 'other', 'those', 'themselves', 'o', 're', 'is', 'had', 'couldn', 'do', 'your', 'herself', 'but', 'won', 'that', 'own', 'why', 'when', 'each', 'on', 'my', 'both', 'you', 'after', 'hers', 'needn', 'how', 'wouldn', \"wouldn't\", 'are', 'ma', 'yourselves', 'we', 'than', \"should've\", 'she', 'once', 'ours', 'nor', \"isn't\", 'hasn', 'from', 'wasn', 'me', 'has', 'same', 'does', 'by', 'y', 'with', \"shan't\", 'were', \"mustn't\", 'our', 'him', 'them', 'before', 'theirs', 'isn', 'its', 'some', \"hasn't\", 'myself', 'if', \"don't\", 'few', \"doesn't\", 'haven', \"couldn't\", 'ourselves', 'i', 'their', 'having', \"haven't\", 'out', \"shouldn't\", 'most', \"didn't\", 'off', 'where', 'for', 'up', 'aren', \"mightn't\", 'mustn', 'or', 'whom', 'and', 'these', 'they', 'then', 'over', 'so', 'am', 'being', 'a', 'further', 'hadn', 'there', 'only', 'not', 'very', \"weren't\", 'who', 'don', 'he', 'any', 'more', 'm', \"that'll\", \"you're\", 'it', 'again', 'itself', 'should', \"wasn't\", 'doing', 'too', 'ain', \"hadn't\", 'which', 'll', 've', 'can', 'through', 'above', 'her', 'an', 's', 'was', 'below', 'of', \"you've\", 'shouldn', 'because', 'until', \"aren't\", 'the', 'at', 'have', 'while', 'weren', 'during', 'such', \"she's\", 'no', 'under', \"it's\", 'his', 'between', 'about', 'in', \"won't\", 'himself', 'just', 'down', 'will', 'now', 'doesn', 'mightn', 'to', 'here', 'yours', \"you'll\", 'd', 'shan', \"needn't\", 't', 'did', 'been', 'all', 'against', \"you'd\", 'yourself', 'didn', 'what', 'into'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
      "['text', 'mine', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(\"Text mining is to identify useful information.\")\n",
    "print(tokens)\n",
    "\n",
    "tokens = stem(tokens)\n",
    "print(tokens)\n",
    "\n",
    "print(filter_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single word is sometimes weakly expressive so that n-gram is a common method about better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n"
     ]
    }
   ],
   "source": [
    "bi_gram = n_gram(tokens, 2)\n",
    "print(bi_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text mine is', 'mine is to', 'is to identifi', 'to identifi use', 'identifi use inform', 'use inform .']\n"
     ]
    }
   ],
   "source": [
    "tri_gram = n_gram(tokens, 3)\n",
    "print(tri_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bag-of-words representation\n",
    "\n",
    "Converting the words to numerical features.\n",
    "\n",
    "### One-hot vector\n",
    "\n",
    "a one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0).\n",
    "\n",
    "For an example sentence `text mining is good`, we map all the words into indexes: map `text` to 0, `mining` to 1, `is` to 2, and `good` to 3.\n",
    "\n",
    "Then the one hot vector for `text` is `[1, 0, 0, 0]`. For `mining` it is `[0, 1, 0, 0]`, and so on.\n",
    "\n",
    "### Bag of word (BOW)\n",
    "\n",
    "The BOW vector of a **sentence** is the sum of all the one-hot vectors.\n",
    "\n",
    "For `text mining is good`, the BOW representation is `[1, 1, 1, 1]`.\n",
    "\n",
    "For `text mining good`, the BOW representation is `[1, 1, 0, 1]`.\n",
    "\n",
    "For `text mining is good mining`, the BOW representation is `[1, 2, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vocabulary_mapping(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of word tokens type: list\n",
    "    \n",
    "    return vocab_dict: a dict from words to indices, type: dict\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    for token in tokens:\n",
    "        if not token in vocab_dict:\n",
    "            vocab_dict[token] = len(vocab_dict)\n",
    "    return vocab_dict\n",
    "\n",
    "def get_bow_vector(tokens, vocab_dict):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokenized words, type: list\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    \n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(vocab_dict), dtype=np.float)\n",
    "    for f in tokens:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = vocab_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = vector[f_idx] + 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocabulary_mapping([\"text\", \"mining\", \"is\", \"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 0, 'mining': 1, 'is': 2, 'good': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 1. 1.]\n",
      "[1. 0. 1. 1.]\n",
      "[1. 0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "tokens_1 = tokenize(\"text mining is good mining.\")\n",
    "tokens_2 = tokenize(\"text is good\")\n",
    "tokens_3 = tokenize(\"good text is good\")\n",
    "print(get_bow_vector(tokens_1, vocab))\n",
    "print(get_bow_vector(tokens_2, vocab))\n",
    "print(get_bow_vector(tokens_3, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more elegant way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bow(corpus):\n",
    "    \"\"\"\n",
    "        :param corpus: a list of strings (type: list)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def vectorize(sentence, vocab):\n",
    "        \"\"\"\n",
    "           :param sentence: a string (type:str)\n",
    "           :param vocab: vocabulary (type:list) \n",
    "           \n",
    "           count function of a list is to count the time of occurance in a list\n",
    "           :return BOW vector\n",
    "        \"\"\"\n",
    "        return [sentence.split().count(i) for i in vocab]\n",
    "\n",
    "    vectorized_corpus = []\n",
    "    vocab = sorted(set([token for doc in corpus for token in doc.lower().split()]))\n",
    "    for sent in corpus:\n",
    "        vectorized_corpus.append((sent, vectorize(sent, vocab)))\n",
    "    return vectorized_corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text mining is good mining', [1, 1, 2, 1]), ('text is good', [1, 1, 0, 1]), ('good text is good', [2, 1, 0, 1])]\n",
      "['good', 'is', 'mining', 'text']\n"
     ]
    }
   ],
   "source": [
    "all_sents = [\"text mining is good mining\",\n",
    "\"text is good\",\n",
    "\"good text is good\"]\n",
    "corpus_bow, vocab = calculate_bow(all_sents)\n",
    "print(corpus_bow)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity\n",
    "\n",
    "$$cos\\_sim(u, v) = \\frac{u \\cdot v}{|u||v|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def cosine_sim(u,v):\n",
    "    return np.dot(u,v) / (sqrt(np.dot(u,u)) * sqrt(np.dot(v,v)))\n",
    "\n",
    "def print_similarity(corpus):\n",
    "    \"\"\"\n",
    "    Print pairwise similarities\n",
    "    \"\"\"\n",
    "    for sentx in corpus:\n",
    "        for senty in corpus:\n",
    "            print(\"{:.4f}\".format(cosine_sim(sentx[1], senty[1])), end=\" \")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000 0.6547 0.6172 \n",
      "0.6547 1.0000 0.9428 \n",
      "0.6172 0.9428 1.0000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similarity(corpus_bow)\n",
    "# [\"text mining is good mining\",\n",
    "# \"text is good\",\n",
    "# \"good text is good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Term weighting\n",
    "- Term frequency\n",
    "- Inverse document frequency\n",
    "- TF-IDF\n",
    "\n",
    "\n",
    "TF:\n",
    "\n",
    "$$\\text{tf}(t, d) = f_{t, d} \\bigg/ \\sum_{t'\\in d} f(t', d)$$\n",
    "\n",
    "Here, $f_{t, d}$ is the number of times term $t$ appearing in document $d$\n",
    "\n",
    "IDF:\n",
    "\n",
    "$$\\text{idf}(t) = N \\big/ n_t$$\n",
    "\n",
    "$N$ is the total number of docs in collection.\n",
    "\n",
    "$n_t$ is the number of docs containing term $t$\n",
    "\n",
    "Examples:\n",
    "```\n",
    "[\n",
    "doc 0: \"text mining is good mining\",\n",
    "doc 1: \"text is good\",\n",
    "doc 2: \"good text is good\"\n",
    "]\n",
    "```\n",
    "For doc 0, `\"good text is good\"`, `tf(\"text\", 0) = 1/4`\n",
    "\n",
    "The IDF of `\"text\"` is `3/3 = 1`\n",
    "\n",
    "The IDF of `\"mining\"` is `3/1 = 3`\n",
    "\n",
    "TF-IDF:\n",
    "\n",
    "$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(freq_dict, term):\n",
    "    \"\"\"\n",
    "        :param freq_dict (type:dict): a dict variable whose key is the word and value is the frequency.\n",
    "            e.g., dict([\"text\":1, \"mining\":1, \"is\":1])\n",
    "        :param term (type:str): the candidate word to calculate TF\n",
    "        \n",
    "        :returns, the TF score of a certain word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return freq_dict[term] / float(sum(freq_dict.values()))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def inverse_doc_freq(freq_dict_list, term):\n",
    "    \"\"\"\n",
    "        :param freq_dict_list (type: list): a list of freq_dict \n",
    "        :param term (type:str): the candidate word to calculate TF\n",
    "        \n",
    "        :returns, the IDF of a certain word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        unique_docs = sum([1 for i,_ in enumerate(freq_dict_list) if freq_dict_list[i][term] > 0])\n",
    "        return float(len(freq_dict_list)) / unique_docs\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow representation: ('good text is good', [2, 1, 0, 1])\n",
      "frequency dict: {'good': 2, 'is': 1, 'mining': 0, 'text': 1}\n"
     ]
    }
   ],
   "source": [
    "# sentence: good text is good\n",
    "current_bow = corpus_bow[2]\n",
    "print(\"bow representation:\", current_bow)\n",
    "doc_vec_dict = {k:v for k,v in zip(vocab, current_bow[1])}\n",
    "print(\"frequency dict:\", doc_vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: good 0.5\n",
      "TF: text 0.25\n",
      "TF: is 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"TF: good\", term_freq(doc_vec_dict, \"good\"))\n",
    "print(\"TF: text\", term_freq(doc_vec_dict, \"text\"))\n",
    "print(\"TF: is\", term_freq(doc_vec_dict, \"is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'good': 1, 'is': 1, 'mining': 2, 'text': 1}, {'good': 1, 'is': 1, 'mining': 0, 'text': 1}, {'good': 2, 'is': 1, 'mining': 0, 'text': 1}]\n"
     ]
    }
   ],
   "source": [
    "freq_dict_list = [{k:v for k,v in zip(vocab, vecs[1])} for vecs in corpus_bow]\n",
    "print(freq_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: good 1.0\n",
      "IDF: text 1.0\n",
      "IDF: is 1.0\n",
      "IDF: mining 3.0\n"
     ]
    }
   ],
   "source": [
    "# all_sents = [\n",
    "# \"text mining is good mining\",\n",
    "# \"text is good\",\n",
    "# \"good text is good\"]\n",
    "print(\"IDF: good\", inverse_doc_freq(freq_dict_list, \"good\"))\n",
    "print(\"IDF: text\", inverse_doc_freq(freq_dict_list, \"text\"))\n",
    "print(\"IDF: is\", inverse_doc_freq(freq_dict_list, \"is\"))\n",
    "print(\"IDF: mining\", inverse_doc_freq(freq_dict_list, \"mining\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus_bow, vocab):\n",
    "    \"\"\"\n",
    "        :params corpus_bow(type: list): the BOW representation of the corpus\n",
    "        :params vocab (type:list): the list of vocab\n",
    "        \n",
    "        return the tf idf representation of the corpus\n",
    "    \"\"\"\n",
    "    word2id = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    freq_dict_list = [{k:v for k,v in zip(vocab, i[1])} for i in corpus_bow]\n",
    "    tfidf_mat  =  np.zeros((len(freq_dict_list), len(vocab)), dtype=float)\n",
    "    for doc_id, doc in enumerate(freq_dict_list):\n",
    "        for term in doc:\n",
    "            term_id = word2id[term]\n",
    "            tf = term_freq(freq_dict_list[doc_id],term)\n",
    "            idf = inverse_doc_freq(freq_dict_list, term)\n",
    "            tfidf_mat[doc_id][term_id] = tf*idf\n",
    "\n",
    "    all_sents = [doc[0] for doc in corpus_bow]\n",
    "    corpus_tfidf = list(zip(all_sents, tfidf_mat))\n",
    "    return corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow_tfidf = calculate_tfidf(corpus_bow, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('text mining is good mining', [1, 1, 2, 1]),\n",
       "  ('text mining is good mining', array([0.2, 0.2, 1.2, 0.2]))),\n",
       " (('text is good', [1, 1, 0, 1]),\n",
       "  ('text is good', array([0.33333333, 0.33333333, 0.        , 0.33333333]))),\n",
       " (('good text is good', [2, 1, 0, 1]),\n",
       "  ('good text is good', array([0.5 , 0.25, 0.  , 0.25])))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(corpus_bow, corpus_bow_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'is', 'mining', 'text']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
