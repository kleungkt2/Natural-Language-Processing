{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use OpenNMT-py to learn an NMT model for the dataset in tutorial 8\n",
    "\n",
    "### 1. Transform to OpenNMT input format:\n",
    "\n",
    "- one sentence per line\n",
    "- seperate for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import *\n",
    "train_path=\"data/fra_cleaned.txt\"\n",
    "valid_path=\"data/fra_cleaned.txt\"\n",
    "\n",
    "max_length = 10\n",
    "train_data = read_translation_pairs_from_file(train_path, max_length=max_length)\n",
    "valid_data = read_translation_pairs_from_file(valid_path, max_length=max_length)\n",
    "valid_data = valid_data[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"translation_data_for_opennmt\")\n",
    "with open(\"translation_data_for_opennmt/train_input.txt\", \"w\") as writer_in, open(\"translation_data_for_opennmt/train_output.txt\", \"w\") as writer_out:\n",
    "    for inputs, outputs in train_data:\n",
    "        writer_in.writelines(\" \".join(inputs)+\"\\n\")\n",
    "        writer_out.writelines(\" \".join(outputs)+\"\\n\")\n",
    "with open(\"translation_data_for_opennmt/valid_input.txt\", \"w\") as writer_in, open(\"translation_data_for_opennmt/valid_output.txt\", \"w\") as writer_out:\n",
    "    for inputs, outputs in valid_data:\n",
    "        writer_in.writelines(\" \".join(inputs)+\"\\n\")\n",
    "        writer_out.writelines(\" \".join(outputs)+\"\\n\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. OpenNMT commands\n",
    "\n",
    "The commands are the APIs for the OpenNMT-py in 2019.10's release.\n",
    "\n",
    "Could be a little bit different than it is now for OpenNMT-py-2.0. The current framework prefers yaml files for defining the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python preprocess.py  \\\n",
    "        -train_src dataset/translation_data_for_opennmt/train_input.txt \\\n",
    "        -train_tgt dataset/translation_data_for_opennmt/train_output.txt \\\n",
    "        -valid_src dataset/translation_data_for_opennmt/valid_output.txt \\\n",
    "        -valid_tgt dataset/translation_data_for_opennmt/valid_input.txt \\\n",
    "        -save_data dataset/translation_data_for_opennmt/translation \\\n",
    "        -src_seq_length 10000 \\\n",
    "        -tgt_seq_length 10000 \\\n",
    "        -src_seq_length_trunc 20 \\\n",
    "        -tgt_seq_length_trunc 20 \\\n",
    "        -shard_size 100000 \\\n",
    "        -src_vocab_size 20000 \\\n",
    "        -tgt_vocab_size 20000 \\\n",
    "        -overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BiRNN encoder and RNN decoder\n",
    "- attention\n",
    "- bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=3 python -u train.py -save_model models/translation_test \\\n",
    "           -data dataset/translation_data_for_opennmt/translation \\\n",
    "           -global_attention mlp \\\n",
    "           -word_vec_size 128 \\\n",
    "           -rnn_size 256 \\\n",
    "           -layers 1 \\\n",
    "           -encoder_type brnn \\\n",
    "           -train_steps 10000 \\\n",
    "           -max_grad_norm 2 \\\n",
    "           -dropout 0. \\\n",
    "           -batch_size 16 \\\n",
    "           -valid_batch_size 16 \\\n",
    "           -optim adagrad \\\n",
    "           -learning_rate 0.15 \\\n",
    "           -adagrad_accumulator_init 0.1 \\\n",
    "           -bridge \\\n",
    "           -seed 229 \\\n",
    "           -world_size 1 \\\n",
    "           -gpu_ranks 0 \\\n",
    "           -valid_steps 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python translate.py -gpu 3 \\\n",
    "     -batch_size 20 \\\n",
    "     -beam_size 4 \\\n",
    "     -model models/translation_test_step_10000.pt \\\n",
    "     -src dataset/translation_data_for_opennmt/valid_input.txt \\\n",
    "     -output valid_decoding.txt \\\n",
    "     -min_length 1 \\\n",
    "     -max_length 15 \\\n",
    "     -verbose "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Checking the decoding results\n",
    "\n",
    "valid_decoding.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8390290942622496\n"
     ]
    }
   ],
   "source": [
    "val_decoding = [line.strip() for line in open(\"./valid_decoding.txt\").readlines()]\n",
    "val_ground = [line.strip() for line in open(\"./translation_data_for_opennmt/valid_output.txt\").readlines()]\n",
    "import nltk\n",
    "def evaluate_bleu(target, output, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "    assert len(target) == len(output)\n",
    "    N = len(target)\n",
    "    \n",
    "    sum_bleu = 0.0\n",
    "    for i in range(N):\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu([target[i]], output[i], weights=weights)\n",
    "        sum_bleu += bleu\n",
    "    return sum_bleu / N\n",
    "print(evaluate_bleu(val_ground, val_decoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_bleu(val_ground, val_ground))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 0.8030787963970216\n"
     ]
    }
   ],
   "source": [
    "print(\"BLEU-4\", evaluate_bleu(val_ground, val_decoding, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better than our naive implementation!\n",
    "\n",
    "Reasons:\n",
    "- The codes are better written\n",
    "- Dedicated attention with better padding and masking mechanisms.\n",
    "- More suitable optimizer: Adagrad turns out to be more efficient than Adam in rnn-based seq2seq models. For transformers, it's better to train with Adam with warming up steps.\n",
    "- bridges: apply an MLP to the last output of the encoder as the input of the decoder.\n",
    "\n",
    "Also, the hyperparameters are *The Chosen Params*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
