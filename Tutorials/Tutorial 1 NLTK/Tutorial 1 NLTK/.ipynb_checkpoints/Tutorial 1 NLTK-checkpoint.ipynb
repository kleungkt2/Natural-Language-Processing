{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK tutorial\n",
    "(From https://www.nltk.org/)\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "We'll talk about the following sections in this tutorial:\n",
    "\n",
    "1. Tokenizer\n",
    "2. Stemmer\n",
    "3. WordNet\n",
    "4. Tips to the assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/tianqing/anaconda3/envs/COMP4901/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: regex in /Users/tianqing/anaconda3/envs/COMP4901/lib/python3.8/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied: click in /Users/tianqing/anaconda3/envs/COMP4901/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /Users/tianqing/anaconda3/envs/COMP4901/lib/python3.8/site-packages (from nltk) (4.49.0)\n",
      "Requirement already satisfied: joblib in /Users/tianqing/anaconda3/envs/COMP4901/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: numpy in /Users/tianqing/anaconda3/envs/COMP4901/lib/python3.8/site-packages (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tianqing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tianqing/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # to make nltk.tokenizer works\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string.split tokenizer ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information.']\n",
      "string.split tokenizer ['Current', 'NLP', 'models', \"isn't\", 'able', 'to', 'solve', 'NLU', 'perfectly.']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Text mining is to identify useful information.\"\n",
    "text2 = \"Current NLP models isn't able to solve NLU perfectly.\"\n",
    "\n",
    "print(\"string.split tokenizer\", text1.split(\" \"))\n",
    "print(\"string.split tokenizer\", text2.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot deal with punctuations, i.e., full stops and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular expression tokenizer ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '']\n",
      "regular expression tokenizer ['Current', 'NLP', 'models', \"isn't\", 'able', 'to', 'solve', 'NLU', 'perfectly', '']\n"
     ]
    }
   ],
   "source": [
    "import regex # regular expression\n",
    "print(\"regular expression tokenizer\", regex.split(\"[\\s\\.]\", text1))\n",
    "print(\"regular expression tokenizer\", regex.split(\"[\\s\\.]\", text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, the `string.split` function can not deal with punctuations\n",
    "- Simple regular expression can deal with most punctuations but may fail in the cases of \"isn't, wasn't, can't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
      "['Current', 'NLP', 'models', 'is', \"n't\", 'able', 'to', 'solve', 'NLU', 'perfectly', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text1))\n",
    "print(tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', '(', 'or', ',', 'NLP', ')', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '...']\n"
     ]
    }
   ],
   "source": [
    "# Other examples:\n",
    "# 1. Possessive cases: Apostrophe (isn't, I've, ...)\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect.\")\n",
    "print(tokens)\n",
    "# 2. Parentheses\n",
    "tokens = tokenize(\"Bob's text mining skills (or, NLP) are perfect.\")\n",
    "print(tokens)\n",
    "# 3. ellipsis\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect...\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stemming and lemmatization\n",
    "\n",
    "(https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "Stemming: chops off the ends of words to acquire the root, and often includes the removal of derivational affixes. \n",
    "\n",
    "e.g., gone -> go, wanted -> want, trees -> tree.\n",
    "\n",
    "Lemmatization: doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . \n",
    "\n",
    "Differences:\n",
    "The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma (focus on the concrete semantic meaning). \n",
    "\n",
    "E.g.: useful -> use(stemming), useful(lemmatization)\n",
    "\n",
    "PorterStemmer:\n",
    "\n",
    "Rule-based methods. E.g., SSES->SS, IES->I, NOUNS->NOUN. # misses->miss, flies->fli.\n",
    "\n",
    "Doc: https://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = stem(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    return [lm.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = lemmatize(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WordNet\n",
    "\n",
    "https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "- a semantically-oriented dictionary of English,\n",
    "- similar to a traditional thesaurus but with a richer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 synsets\n",
    "\n",
    "A set of one or more **synonyms** that are interchangeable in some context without changing the truth value of the proposition in which they are embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up a word using synsets(); \n",
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10'),\n",
       " Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synset \t definition\n",
      "Synset('bank.n.01') \t sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') \t a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') \t a long ridge or pile\n",
      "Synset('bank.n.04') \t an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') \t a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') \t the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') \t a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') \t a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') \t a building in which the business of banking transacted\n",
      "Synset('bank.n.10') \t a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') \t tip laterally\n",
      "Synset('bank.v.02') \t enclose with a bank\n",
      "Synset('bank.v.03') \t do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') \t act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') \t be in the banking business\n",
      "Synset('deposit.v.02') \t put into a bank account\n",
      "Synset('bank.v.07') \t cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') \t have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "print(\"synset\",\"\\t\",\"definition\")\n",
    "for synset in wn.synsets('bank'):\n",
    "    print(synset, '\\t', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function has an optional pos argument which lets you constrain the part of speech of the word:\n",
    "# pos: part-of-speech\n",
    "wn.synsets('bank', pos=wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the dog barked all night']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'domestic_dog', 'Canis_familiaris']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(wn.synset('dog.n.01'))\n",
    "# isA: hyponyms, hypernyms\n",
    "# part_of: member_holonyms, substance_holonyms, part_holonyms\n",
    "# being part of: member_meronyms, substance_meronyms, part_meronyms\n",
    "# domains: topic_domains, region_domains, usage_domains\n",
    "# attribute: attributes\n",
    "# entailments: entailments\n",
    "# causes: causes\n",
    "# also_sees: also_sees\n",
    "# verb_groups: verb_groups\n",
    "# similar_to: similar_tos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check more relations in http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "hyponyms: [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# hypernyms: abstraction\n",
    "# hyponyms: instantiation\n",
    "\n",
    "dog = wn.synset('dog.n.01')\n",
    "print(\"hypernyms:\", dog.hypernyms())\n",
    "print(\"hyponyms:\", dog.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('carnivore.n.01')]\n",
      "[Synset('placental.n.01')]\n",
      "[Synset('mammal.n.01')]\n",
      "root hypernyms for dog: [Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(dog.hypernyms()[0].hypernyms()) # the hypernym of canine\n",
    "# animals that feeds on flesh\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of carnivore\n",
    "# placental mammals\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of placental\n",
    "# mammals\n",
    "# ...\n",
    "print(\"root hypernyms for dog:\", dog.root_hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root hypernyms for cat: [Synset('feline.n.01')]\n",
      "root hypernyms for cat: [Synset('entity.n.01')]\n",
      "the lowest common hypernyms of dog and cat\n",
      "[Synset('carnivore.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# find common hypernyms\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').hypernyms())\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').root_hypernyms())\n",
    "print(\"the lowest common hypernyms of dog and cat\")\n",
    "print(wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('cat.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "corgi = wn.synset('corgi.n.01')\n",
    "bensenji = wn.synset('basenji.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat) # dog <- canine <- carnivore -> feline -> cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(corgi) # corgi <- dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corgi.path_similarity(bensenji) # bensenji <- dog -> corgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')\n",
    "jump = wn.synset('jump.v.01')\n",
    "run = wn.synset('run.v.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(slap) # 1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(jump) # 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also check:\n",
    "- wup_similarity\n",
    "- lch_similarity\n",
    "- res_similarity\n",
    "...\n",
    "\n",
    "Find more on https://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Traverse the synsets to build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_graph_hypernyms = {}\n",
    "# or you could use networkx package\n",
    "\n",
    "for synset in list(wn.all_synsets('n'))[:10]:\n",
    "    for hyp_syn in synset.hypernyms():\n",
    "        wn_graph_hypernyms[synset.name()] = {**wn_graph_hypernyms.get(synset.name(), {}), **{hyp_syn.name():True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_graph_hypernyms['physical_entity.n.01']['entity.n.01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tips to the assignments\n",
    "\n",
    "Some corpus in the NLTK.\n",
    "\n",
    "Reference: https://www.nltk.org/book/ch02.html. You could search for `gutenberg` and `brown` for detailed documentations.\n",
    "\n",
    "### 4.1 gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/tianqing/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg as gb\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = 'austen-sense.txt'\n",
    "word_list = gb.words(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']', 'CHAPTER', '1', 'The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.', 'Their', 'estate', 'was', 'large', ',', 'and', 'their', 'residence', 'was', 'at', 'Norland', 'Park', ',', 'in', 'the', 'centre', 'of', 'their', 'property', ',', 'where', ',', 'for', 'many', 'generations', ',', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'a', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', '.', 'The', 'late', 'owner', 'of', 'this', 'estate', 'was', 'a', 'single', 'man', ',', 'who', 'lived', 'to', 'a', 'very', 'advanced', 'age', ',', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', ',', 'had', 'a', 'constant', 'companion']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gb.sents(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/tianqing/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download(\"brown\")\n",
    "print(brown.categories())\n",
    "\n",
    "romance_word_list = brown.words(categories='romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', 'neither', 'liked', 'nor', 'disliked', 'the', ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romance_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
