{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK tutorial\n",
    "(From https://www.nltk.org/)\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "We'll talk about the following sections in this tutorial:\n",
    "\n",
    "1. Tokenizer\n",
    "2. Stemmer\n",
    "3. WordNet\n",
    "4. Tips to the assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\elva\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.7.14-cp38-cp38-win_amd64.whl (264 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434680 sha256=58bfd6b52d33ffc7a235bc3c7ff0fdeecdea11b521b6384fc919bb0b484ad4f2\n",
      "  Stored in directory: c:\\users\\elva\\appdata\\local\\pip\\cache\\wheels\\ff\\d5\\7b\\f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.7.14 tqdm-4.49.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Elva\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\elva\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Elva\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Elva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # to make nltk.tokenizer works\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string.split tokenizer ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information.']\n",
      "string.split tokenizer ['Current', 'NLP', 'models', \"isn't\", 'able', 'to', 'solve', 'NLU', 'perfectly.']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Text mining is to identify useful information.\"\n",
    "text2 = \"Current NLP models isn't able to solve NLU perfectly.\"\n",
    "\n",
    "print(\"string.split tokenizer\", text1.split(\" \"))\n",
    "print(\"string.split tokenizer\", text2.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot deal with punctuations, i.e., full stops and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular expression tokenizer ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '']\n",
      "regular expression tokenizer ['Current', 'NLP', 'models', \"isn't\", 'able', 'to', 'solve', 'NLU', 'perfectly', '']\n"
     ]
    }
   ],
   "source": [
    "import regex # regular expression\n",
    "print(\"regular expression tokenizer\", regex.split(\"[\\s\\.]\", text1))\n",
    "print(\"regular expression tokenizer\", regex.split(\"[\\s\\.]\", text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, the `string.split` function can not deal with punctuations\n",
    "- Simple regular expression can deal with most punctuations but may fail in the cases of \"isn't, wasn't, can't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
      "['Current', 'NLP', 'models', 'is', \"n't\", 'able', 'to', 'solve', 'NLU', 'perfectly', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text1))\n",
    "print(tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', '(', 'or', ',', 'NLP', ')', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '...']\n"
     ]
    }
   ],
   "source": [
    "# Other examples:\n",
    "# 1. Possessive cases: Apostrophe (isn't, I've, ...)\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect.\")\n",
    "print(tokens)\n",
    "# 2. Parentheses\n",
    "tokens = tokenize(\"Bob's text mining skills (or, NLP) are perfect.\")\n",
    "print(tokens)\n",
    "# 3. ellipsis\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect...\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stemming and lemmatization\n",
    "\n",
    "(https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "Stemming: chops off the ends of words to acquire the root, and often includes the removal of derivational affixes. \n",
    "\n",
    "e.g., gone -> go, wanted -> want, trees -> tree.\n",
    "\n",
    "Lemmatization: doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . \n",
    "\n",
    "Differences:\n",
    "The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma (focus on the concrete semantic meaning). \n",
    "\n",
    "E.g.: useful -> use(stemming), useful(lemmatization)\n",
    "\n",
    "PorterStemmer:\n",
    "\n",
    "Rule-based methods. E.g., SSES->SS, IES->I, NOUNS->NOUN. # misses->miss, flies->fli.\n",
    "\n",
    "Doc: https://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = stem(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    return [lm.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = lemmatize(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WordNet\n",
    "\n",
    "https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "- a semantically-oriented dictionary of English,\n",
    "- similar to a traditional thesaurus but with a richer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 synsets\n",
    "\n",
    "A set of one or more **synonyms** that are interchangeable in some context without changing the truth value of the proposition in which they are embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up a word using synsets(); \n",
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10'),\n",
       " Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synset \t definition\n",
      "Synset('bank.n.01') \t sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') \t a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') \t a long ridge or pile\n",
      "Synset('bank.n.04') \t an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') \t a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') \t the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') \t a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') \t a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') \t a building in which the business of banking transacted\n",
      "Synset('bank.n.10') \t a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') \t tip laterally\n",
      "Synset('bank.v.02') \t enclose with a bank\n",
      "Synset('bank.v.03') \t do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') \t act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') \t be in the banking business\n",
      "Synset('deposit.v.02') \t put into a bank account\n",
      "Synset('bank.v.07') \t cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') \t have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "print(\"synset\",\"\\t\",\"definition\")\n",
    "for synset in wn.synsets('bank'):\n",
    "    print(synset, '\\t', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function has an optional pos argument which lets you constrain the part of speech of the word:\n",
    "# pos: part-of-speech\n",
    "wn.synsets('bank', pos=wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the dog barked all night']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'domestic_dog', 'Canis_familiaris']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_all_hypernyms',\n",
       " '_definition',\n",
       " '_examples',\n",
       " '_frame_ids',\n",
       " '_hypernyms',\n",
       " '_instance_hypernyms',\n",
       " '_iter_hypernym_lists',\n",
       " '_lemma_names',\n",
       " '_lemma_pointers',\n",
       " '_lemmas',\n",
       " '_lexname',\n",
       " '_max_depth',\n",
       " '_min_depth',\n",
       " '_name',\n",
       " '_needs_root',\n",
       " '_offset',\n",
       " '_pointers',\n",
       " '_pos',\n",
       " '_related',\n",
       " '_shortest_hypernym_paths',\n",
       " '_wordnet_corpus_reader',\n",
       " 'also_sees',\n",
       " 'attributes',\n",
       " 'causes',\n",
       " 'closure',\n",
       " 'common_hypernyms',\n",
       " 'definition',\n",
       " 'entailments',\n",
       " 'examples',\n",
       " 'frame_ids',\n",
       " 'hypernym_distances',\n",
       " 'hypernym_paths',\n",
       " 'hypernyms',\n",
       " 'hyponyms',\n",
       " 'in_region_domains',\n",
       " 'in_topic_domains',\n",
       " 'in_usage_domains',\n",
       " 'instance_hypernyms',\n",
       " 'instance_hyponyms',\n",
       " 'jcn_similarity',\n",
       " 'lch_similarity',\n",
       " 'lemma_names',\n",
       " 'lemmas',\n",
       " 'lexname',\n",
       " 'lin_similarity',\n",
       " 'lowest_common_hypernyms',\n",
       " 'max_depth',\n",
       " 'member_holonyms',\n",
       " 'member_meronyms',\n",
       " 'min_depth',\n",
       " 'name',\n",
       " 'offset',\n",
       " 'part_holonyms',\n",
       " 'part_meronyms',\n",
       " 'path_similarity',\n",
       " 'pos',\n",
       " 'region_domains',\n",
       " 'res_similarity',\n",
       " 'root_hypernyms',\n",
       " 'shortest_path_distance',\n",
       " 'similar_tos',\n",
       " 'substance_holonyms',\n",
       " 'substance_meronyms',\n",
       " 'topic_domains',\n",
       " 'tree',\n",
       " 'usage_domains',\n",
       " 'verb_groups',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn.synset('dog.n.01'))\n",
    "# isA: hyponyms, hypernyms\n",
    "# part_of: member_holonyms, substance_holonyms, part_holonyms\n",
    "# being part of: member_meronyms, substance_meronyms, part_meronyms\n",
    "# domains: topic_domains, region_domains, usage_domains\n",
    "# attribute: attributes\n",
    "# entailments: entailments\n",
    "# causes: causes\n",
    "# also_sees: also_sees\n",
    "# verb_groups: verb_groups\n",
    "# similar_to: similar_tos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check more relations in http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "hyponyms: [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# hypernyms: abstraction\n",
    "# hyponyms: instantiation\n",
    "\n",
    "dog = wn.synset('dog.n.01')\n",
    "print(\"hypernyms:\", dog.hypernyms())\n",
    "print(\"hyponyms:\", dog.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('carnivore.n.01')]\n",
      "[Synset('placental.n.01')]\n",
      "[Synset('mammal.n.01')]\n",
      "root hypernyms for dog: [Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(dog.hypernyms()[0].hypernyms()) # the hypernym of canine\n",
    "# animals that feeds on flesh\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of carnivore\n",
    "# placental mammals\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of placental\n",
    "# mammals\n",
    "# ...\n",
    "print(\"root hypernyms for dog:\", dog.root_hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root hypernyms for cat: [Synset('feline.n.01')]\n",
      "root hypernyms for cat: [Synset('entity.n.01')]\n",
      "the lowest common hypernyms of dog and cat\n",
      "[Synset('carnivore.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# find common hypernyms\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').hypernyms())\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').root_hypernyms())\n",
    "print(\"the lowest common hypernyms of dog and cat\")\n",
    "print(wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('cat.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "corgi = wn.synset('corgi.n.01')\n",
    "bensenji = wn.synset('basenji.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat) # dog <- canine <- carnivore -> feline -> cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(corgi) # corgi <- dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corgi.path_similarity(bensenji) # bensenji <- dog -> corgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')\n",
    "jump = wn.synset('jump.v.01')\n",
    "run = wn.synset('run.v.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(slap) # 1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(jump) # 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also check:\n",
    "- wup_similarity\n",
    "- lch_similarity\n",
    "- res_similarity\n",
    "...\n",
    "\n",
    "Find more on https://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Traverse the synsets to build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_graph_hypernyms = {}\n",
    "# or you could use networkx package\n",
    "\n",
    "for synset in list(wn.all_synsets('n'))[:10]:\n",
    "    for hyp_syn in synset.hypernyms():\n",
    "        wn_graph_hypernyms[synset.name()] = {**wn_graph_hypernyms.get(synset.name(), {}), **{hyp_syn.name():True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_graph_hypernyms['physical_entity.n.01']['entity.n.01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tips to the assignments\n",
    "\n",
    "Some corpus in the NLTK.\n",
    "\n",
    "Reference: https://www.nltk.org/book/ch02.html. You could search for `gutenberg` and `brown` for detailed documentations.\n",
    "\n",
    "### 4.1 gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Elva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg as gb\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = 'austen-sense.txt'\n",
    "word_list = gb.words(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']', 'CHAPTER', '1', 'The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.', 'Their', 'estate', 'was', 'large', ',', 'and', 'their', 'residence', 'was', 'at', 'Norland', 'Park', ',', 'in', 'the', 'centre', 'of', 'their', 'property', ',', 'where', ',', 'for', 'many', 'generations', ',', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'a', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', '.', 'The', 'late', 'owner', 'of', 'this', 'estate', 'was', 'a', 'single', 'man', ',', 'who', 'lived', 'to', 'a', 'very', 'advanced', 'age', ',', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', ',', 'had', 'a', 'constant', 'companion']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gb.sents(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Elva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download(\"brown\")\n",
    "print(brown.categories())\n",
    "\n",
    "romance_word_list = brown.words(categories='romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'neither',\n",
       " 'liked',\n",
       " 'nor',\n",
       " 'disliked',\n",
       " 'the',\n",
       " 'Old',\n",
       " 'Man',\n",
       " '.',\n",
       " 'To',\n",
       " 'them',\n",
       " 'he',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'the',\n",
       " 'broken',\n",
       " 'bell',\n",
       " 'in',\n",
       " 'the',\n",
       " 'church',\n",
       " 'tower',\n",
       " 'which',\n",
       " 'rang',\n",
       " 'before',\n",
       " 'and',\n",
       " 'after',\n",
       " 'Mass',\n",
       " ',',\n",
       " 'and',\n",
       " 'at',\n",
       " 'noon',\n",
       " ',',\n",
       " 'and',\n",
       " 'at',\n",
       " 'six',\n",
       " 'each',\n",
       " 'evening',\n",
       " '--',\n",
       " 'its',\n",
       " 'tone',\n",
       " ',',\n",
       " 'repetitive',\n",
       " ',',\n",
       " 'monotonous',\n",
       " ',',\n",
       " 'never',\n",
       " 'breaking',\n",
       " 'the',\n",
       " 'boredom']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romance_word_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1: \n",
      "1. Print the number of word tokens in the corpus:\n",
      "141576\n",
      "\n",
      "2. Print the size of the vocabulary (number of unique word tokens).\n",
      "6833\n",
      "\n",
      "3. Print the tokenized words of the first sentence in the corpus.\n",
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']']\n",
      "\n",
      "q2: \n",
      "1. Print the top 10 most common words in the romance category:\n",
      "[',', '.', 'the', 'and', 'to', 'a', 'of', '``', \"''\", 'was']\n",
      "\n",
      "2. Print the word frequencies:\n",
      "frequency of ring in romance: 2\n",
      "frequency of activities in romance: 1\n",
      "frequency of love in romance: 32\n",
      "frequency of sports in romance: 3\n",
      "frequency of church in romance: 29\n",
      "\n",
      "frequency of ring in hobbies: 11\n",
      "frequency of activities in hobbies: 11\n",
      "frequency of love in hobbies: 6\n",
      "frequency of sports in hobbies: 12\n",
      "frequency of church in hobbies: 1\n",
      "\n",
      "q3: \n",
      "1. All synonymous words of the word 'dictionary':\n",
      "['dictionary', 'lexicon']\n",
      "\n",
      "2. Hyponyms of the word 'dictionary':\n",
      "[Synset('bilingual_dictionary.n.01'), Synset('desk_dictionary.n.01'), Synset('etymological_dictionary.n.01'), Synset('gazetteer.n.02'), Synset('learner's_dictionary.n.01'), Synset('pocket_dictionary.n.01'), Synset('spell-checker.n.01'), Synset('unabridged_dictionary.n.01')]\n",
      "\n",
      "3.Calculate similarities:\n",
      "Similarity of 'right_whale' and 'minke_whale': 0.25\n",
      "Similarity of 'right_whale' and 'tortoise': 0.07692307692307693\n",
      "Similarity of 'right_whale' and 'novel': 0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def q1():\n",
    "    print('q1: {:}'.format(''))\n",
    "    from nltk.corpus import gutenberg as gb\n",
    "    import nltk\n",
    "    file_id = 'austen-sense.txt'\n",
    "    word_list = gb.words(file_id)\n",
    "    # YOUR CODE\n",
    "    # 1. Print the number of word tokens in the corpus.\n",
    "    print(\"1. Print the number of word tokens in the corpus:\")\n",
    "    print(len(word_list))\n",
    "    # 2. Print the size of the vocabulary (number of unique word tokens).\n",
    "    print()\n",
    "    print(\"2. Print the size of the vocabulary (number of unique word tokens).\")\n",
    "    print(len(set(word_list)))\n",
    "    print()\n",
    "    print(\"3. Print the tokenized words of the first sentence in the corpus.\")\n",
    "    # 3. Print the tokenized words of the first sentence in the corpus.\n",
    "    first_sents = gb.sents(file_id)[0]\n",
    "    s = \" \".join(first_sents)\n",
    "    print(nltk.word_tokenize(s))\n",
    "def q2():\n",
    "    print('q2: {:}'.format(''))\n",
    "    import nltk\n",
    "    from nltk.corpus import brown\n",
    "    # Your Code\n",
    "    romance_word_list = brown.words(categories='romance')\n",
    "    # 1. Print the top 10 most common words in the romance category.\n",
    "    print(\"1. Print the top 10 most common words in the romance category:\")\n",
    "    # 2. Print the word frequencies\n",
    "    from collections import Counter\n",
    "    words_given = ['ring','activities','love','sports','church']\n",
    "    counter =  Counter(romance_word_list)\n",
    "    most_common = counter.most_common(10)\n",
    "    word_list = []\n",
    "    for word,freq in most_common:\n",
    "        word_list.append(word)\n",
    "    print(word_list)\n",
    "    print()\n",
    "    print(\"2. Print the word frequencies:\")\n",
    "    for word in words_given:\n",
    "        print('frequency of ' + word + ' in romance: ' + str(counter[word]))\n",
    "    hobbies_word_list = brown.words(categories='hobbies')\n",
    "    counter = Counter(hobbies_word_list)\n",
    "    print()\n",
    "    for word in words_given:\n",
    "        print('frequency of ' + word + ' in hobbies: ' + str(counter[word])) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def q3():\n",
    "    print('q3: {:}'.format(''))\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    # Your Code\n",
    "    # 1. Print all synonymous words of the word ‘dictionary’.\n",
    "    print('1. All synonymous words of the word \\'dictionary\\':')\n",
    "    print(wn.synset('dictionary.n.01').lemma_names())\n",
    "    print()\n",
    "    # 2. Print all hyponyms of the word ‘dictionary’.\n",
    "    print(\"2. Hyponyms of the word \\'dictionary\\':\")\n",
    "    dictionary = wn.synset('dictionary.n.01')\n",
    "    hyp = dictionary.hyponyms()\n",
    "    print(hyp)\n",
    "    print()\n",
    "    # 3. Calculate similarities.\n",
    "    sim = []\n",
    "    right_whale = wn.synset('right_whale.n.01')\n",
    "    novel = wn.synset('novel.n.01')\n",
    "    minke_whale = wn.synset('minke_whale.n.01')\n",
    "    tortoise = wn.synset('tortoise.n.01')\n",
    "    \n",
    "    pair1 = right_whale.path_similarity(novel)\n",
    "    pair2 = right_whale.path_similarity(minke_whale)\n",
    "    pair3 = right_whale.path_similarity(tortoise)\n",
    "    print(\"3.Calculate similarities:\")\n",
    "    print(\"Similarity of \\'right_whale\\' and \\'minke_whale\\': \" + str(pair2))\n",
    "    print(\"Similarity of \\'right_whale\\' and \\'tortoise\\': \" + str(pair3))\n",
    "    print(\"Similarity of \\'right_whale\\' and \\'novel\\': \" + str(pair1))\n",
    "q1()\n",
    "print()\n",
    "q2()\n",
    "print()\n",
    "q3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
