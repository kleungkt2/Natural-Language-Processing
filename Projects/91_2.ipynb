{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPBX7aUJ5Oki"
   },
   "source": [
    "# Load dataset\n",
    "\n",
    "Data format:\n",
    "\n",
    "|id|word_seq|tag_seq|\n",
    "|:--|:--|:--|\n",
    "|index of the sentence|tokenized words|corresponding NER tags|\n",
    "|0|`[\"protection\", \"calves\", ...]`|`[\"O\", \"LIVESTOCK\", ...]`|\n",
    "|1|`[\"prevent\", \"diarrhea\",...]` |`[\"O\", \"DISEASE_OR_SYNDROME\", ...]`|\n",
    "|...|...|...|\n",
    "\n",
    "\n",
    "\n",
    "There are 64 categories of NER tags (plus 1 padding token).\n",
    "\n",
    "The ground-truth tags are provided for the training and testing set, while being omitted in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIr39GTE5Okq",
    "outputId": "a4dd5d29-4162-424c-fb88-b514d99450d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in train_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in val_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in test_dict: dict_keys(['id', 'word_seq'])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "train_dict = pkl.load(open(\"data/train.pkl\", \"rb\"))\n",
    "val_dict = pkl.load(open(\"data/val.pkl\", \"rb\"))\n",
    "test_dict = pkl.load(open(\"data/test.pkl\", \"rb\"))\n",
    "print(\"keys in train_dict:\", train_dict.keys())\n",
    "print(\"keys in val_dict:\", val_dict.keys())\n",
    "print(\"keys in test_dict:\", test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4p0ItcG5Ok6",
    "outputId": "7d9a84cf-9a48-4227-f468-13efceef635c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "('Protection', 'O') ('of', 'O') ('calves', 'LIVESTOCK') ('against', 'O') ('fatal', 'O') ('enteric', 'DISEASE_OR_SYNDROME') ('colibacillosis', 'DISEASE_OR_SYNDROME') ('by', 'O') ('orally', 'GENE_OR_GENOME') ('administered', 'GENE_OR_GENOME') ('Escherichia', 'GENE_OR_GENOME') ('coli', 'GENE_OR_GENOME') ('K99', 'GENE_OR_GENOME') ('-', 'O') ('specific', 'CARDINAL') ('monoclonal', 'CARDINAL') ('antibody', 'CARDINAL') ('.', 'O') ('A', 'O') ('monoclonal', 'CHEMICAL') ('antibody', 'CHEMICAL') ('(', 'O') ('MCA', 'GENE_OR_GENOME') (')', 'O') ('to', 'O') ('enterotoxigenic', 'CHEMICAL') ('Escherichia', 'CHEMICAL') ('coli', 'CHEMICAL') ('K99', 'O') ('antigen', 'O') ('agglutinated', 'O') ('K99+', 'GENE_OR_GENOME') ('enterotoxigenic', 'GENE_OR_GENOME') ('E', 'GENE_OR_GENOME') ('.', 'O') ('coli', 'CHEMICAL') ('strains', 'CHEMICAL') ('B44', 'CHEMICAL') ('(', 'O') ('O9', 'O') (':', 'O') ('K30', 'O') (';', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('and', 'O') ('B41', 'CHEMICAL') ('(', 'O') ('O101', 'PRODUCT') (':', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('grown', 'O') ('at', 'O') ('37', 'QUANTITY') ('degrees', 'QUANTITY') ('C', 'O') ('but', 'O') ('not', 'O') ('at', 'O') ('18', 'QUANTITY') ('degrees', 'QUANTITY') ('C.', 'O') ('The', 'O') ('MCA', 'GENE_OR_GENOME') (',', 'O') ('which', 'O') ('was', 'O') ('characterized', 'O') ('as', 'O') ('immunoglobulin', 'GENE_OR_GENOME') ('G1', 'GENE_OR_GENOME') (',', 'O') ('reacted', 'O') ('specifically', 'O') ('with', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('in', 'O') ('an', 'O') ('enzyme-linked', 'CHEMICAL') ('immunosorbent', 'CHEMICAL') ('assay', 'CHEMICAL') ('and', 'O') ('precipitated', 'O') ('radiolabeled', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('.', 'O') ('A', 'O') ('total', 'O') ('of', 'O') ('45', 'O') ('colostrum', 'CHEMICAL') ('-fed', 'O') ('and', 'O') ('colostrum', 'CHEMICAL') ('-deprived', 'O') ('calves', 'LIVESTOCK') ('were', 'O') ('used', 'O') ('in', 'O') ('three', 'CARDINAL') ('separate', 'O') ('trials', 'O') ('to', 'O') ('determine', 'O') ('whether', 'O') ('the', 'O') ('orally', 'O') ('administered', 'O') ('K99-specific', 'O') ('MCA', 'GENE_OR_GENOME') ('would', 'O') ('prevent', 'O') ('diarrhea', 'DISEASE_OR_SYNDROME') ('caused', 'O') ('by', 'O') ('strain', 'O') ('B44', 'GENE_OR_GENOME')\n"
     ]
    }
   ],
   "source": [
    "# an entry of the dataset\n",
    "print(\"index:\", train_dict[\"id\"][0])\n",
    "print(*zip(train_dict[\"word_seq\"][0], train_dict[\"tag_seq\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5I64P4k75OlG",
    "outputId": "b6f63caf-e6f3-4ea0-d8f4-059b71f50219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of the NER tags: 65\n",
      "all the NER tags: {'MACHINE_ACTIVITY', 'CELL', 'DISEASE_OR_SYNDROME', 'INJURY_OR_POISONING', 'EUKARYOTE', 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE', 'MONEY', 'PERCENT', 'GENE_OR_GENOME', 'EDUCATIONAL_ACTIVITY', 'LOC', 'WORK_OF_ART', 'ARCHAEON', 'RESEARCH_ACTIVITY', 'CARDINAL', 'LABORATORY_PROCEDURE', 'CELL_COMPONENT', 'EVOLUTION', 'LABORATORY_OR_TEST_RESULT', 'DAILY_OR_RECREATIONAL_ACTIVITY', 'BODY_SUBSTANCE', 'LANGUAGE', 'BACTERIUM', 'TISSUE', 'SOCIAL_BEHAVIOR', 'DIAGNOSTIC_PROCEDURE', 'CELL_OR_MOLECULAR_DYSFUNCTION', 'BODY_PART_ORGAN_OR_ORGAN_COMPONENT', 'O', 'EVENT', 'MOLECULAR_FUNCTION', 'CELL_FUNCTION', 'SUBSTRATE', 'GOVERNMENTAL_OR_REGULATORY_ACTIVITY', 'VIRUS', 'PERSON', 'FAC', 'GPE', 'LAW', 'LIVESTOCK', 'TIME', 'EXPERIMENTAL_MODEL_OF_DISEASE', 'PHYSICAL_SCIENCE', 'DATE', 'QUANTITY', 'VIRAL_PROTEIN', 'PRODUCT', 'FOOD', 'CHEMICAL', 'CORONAVIRUS', 'SIGN_OR_SYMPTOM', 'GROUP_ATTRIBUTE', 'NORP', 'GROUP', 'ORGAN_OR_TISSUE_FUNCTION', 'MATERIAL', 'IMMUNE_RESPONSE', 'ORGANISM', 'HUMAN-CAUSED_PHENOMENON_OR_PROCESS', 'ANATOMICAL_STRUCTURE', 'INDIVIDUAL_BEHAVIOR', 'WILDLIFE', '_t_pad_', 'ORG', 'ORDINAL'}\n"
     ]
    }
   ],
   "source": [
    "# all the NER tags:\n",
    "from itertools import chain\n",
    "print(\"count of the NER tags:\", len(set(chain(*train_dict[\"tag_seq\"]))))\n",
    "print(\"all the NER tags:\", set(chain(*train_dict[\"tag_seq\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSTqhoWO5OlM"
   },
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOC17Def5OlO",
    "outputId": "da2114a3-6fa3-472d-c1d8-c0fd103f9e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of word vocab: 82275 size of tag_dict: 65\n"
     ]
    }
   ],
   "source": [
    "# prepare word vocab and tag vocab\n",
    "\n",
    "vocab_dict = {'_unk_': 0, '_w_pad_': 1}\n",
    "\n",
    "for doc in train_dict['word_seq']:\n",
    "    for word in doc:\n",
    "        if(word not in vocab_dict):\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "\n",
    "tag_dict = {'_t_pad_': 0} # add a padding token\n",
    "\n",
    "for tag_seq in train_dict['tag_seq']:\n",
    "    for tag in tag_seq:\n",
    "        if(tag not in tag_dict):\n",
    "            tag_dict[tag] = len(tag_dict)\n",
    "word2idx = vocab_dict\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "tag2idx = tag_dict\n",
    "idx2tag = {v:k for k,v in tag2idx.items()}            \n",
    "\n",
    "print(\"size of word vocab:\", len(vocab_dict), \"size of tag_dict:\", len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4SBSBxUa5OlV"
   },
   "outputs": [],
   "source": [
    "# The maximum length of a sentence is set to 128\n",
    "max_sent_length = 128\n",
    "\n",
    "train_tokens = np.array([[word2idx[w] for w in doc] for doc in train_dict['word_seq']])\n",
    "val_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in val_dict['word_seq']])\n",
    "test_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in test_dict['word_seq']])\n",
    "\n",
    "\n",
    "train_tags = [[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']]\n",
    "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])\n",
    "\n",
    "val_tags = [[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']]\n",
    "val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])\n",
    "\n",
    "# we don't have test tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wolo3Asu5Olc",
    "outputId": "1d927d3e-d73a-424d-956e-eb2303b1cfa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (23600, 128) tag size: (23600, 128, 65)\n",
      "validating size: (2950, 128) tag size: (2950, 128, 65)\n"
     ]
    }
   ],
   "source": [
    "print(\"training size:\", train_tokens.shape, \"tag size:\", train_tags.shape)\n",
    "print(\"validating size:\", val_tokens.shape, \"tag size:\", val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmMYYZ995Olj",
    "outputId": "0b319ca4-6a56-4a1e-8168-a5bad59fbbb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11] [1 1 2 1 1 3 3 1 4 4]\n"
     ]
    }
   ],
   "source": [
    "# an example of training instance and training tags.\n",
    "print(train_tokens[0,:10], np.argmax(train_tags[0, :10, :], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ocVGRZcu5Oln"
   },
   "outputs": [],
   "source": [
    "num_training_data = train_tokens.shape[0]\n",
    "sequence_length = train_tokens.shape[1]\n",
    "vocabulary_size = len(vocab_dict)\n",
    "num_tags = len(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "totEcVy25Ol1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\\\n",
    "    Bidirectional, SimpleRNN, LSTM, GRU, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bICaQwdg5Ol6"
   },
   "outputs": [],
   "source": [
    "def build_RNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_rnn_layers: the number of layers of the RNN, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param rnn_type: the type of RNN, type: str\n",
    "    :param bidirectional: whether to use bidirectional rnn, type: bool\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a RNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # recurrent layers document: https://keras.io/layers/recurrent\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    \n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ####### Recurrent Layers #######\n",
    "    ################################\n",
    "    # recurrent layers\n",
    "    # Referennce: https://keras.io/api/layers/#recurrent-layers\n",
    "    if rnn_type == \"rnn\":\n",
    "        fn = SimpleRNN\n",
    "    elif rnn_type == \"lstm\":\n",
    "        fn = LSTM\n",
    "    elif rnn_type == \"gru\":\n",
    "        fn = GRU\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    h = emb\n",
    "    for i in range(num_rnn_layers):\n",
    "        is_last = (i == num_rnn_layers-1)\n",
    "        if bidirectional:\n",
    "            h = Bidirectional(fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=True))(h)\n",
    "            # return_sequences:\n",
    "            # Boolean. Whether to return the last output. in the output sequence, or the full sequence.\n",
    "            # [h_1, h_2, ..., h_n] or h_n\n",
    "        else:\n",
    "            h = fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=not is_last)(h)\n",
    "        h = Dropout(dropout_rate, seed=0)(h)\n",
    "    \n",
    "    ################################\n",
    "    #### Fully Connected Layers ####\n",
    "    ################################\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                      bias_initializer=\"zeros\",\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add residual connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\",\n",
    "              kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "              bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCoXBn0c5OmR",
    "outputId": "47a20554-7e69-4552-dfef-c0dd1e136760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 76/213 [=========>....................] - ETA: 1:05 - loss: 1.0335 - accuracy: 0.7771"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "hidden_size = 64\n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model = build_RNN(max_sent_length, vocabulary_size, embedding_size,\n",
    "              hidden_size, num_tags,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.3,\n",
    "              batch_norm=True,\n",
    "              l2_reg=0.3,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.01,\n",
    "              metric=\"accuracy\")\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=0)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=10, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Rm0X2ZjlUV2c"
   },
   "outputs": [],
   "source": [
    "# val set\n",
    "val_preds = model.predict(val_tokens)\n",
    "val_preds_id = np.argmax(val_preds, axis=2)\n",
    "val_preds_labels = np.array([[idx2tag[p] for p in preds1] for preds1 in val_preds_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fmq8rOSM5Omm"
   },
   "outputs": [],
   "source": [
    "# test set\n",
    "preds = model.predict(test_tokens)\n",
    "preds_id = np.argmax(preds, axis=2)\n",
    "preds_labels = np.array([[idx2tag[p] for p in preds1] for preds1 in preds_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uRpzXOA_5Om3"
   },
   "outputs": [],
   "source": [
    "val_tags_by_idx = np.argmax(val_tags, axis=2)\n",
    "val_labels = np.array([[idx2tag[p] for p in preds] for preds in val_tags_by_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y3MxxJzP5Om7"
   },
   "outputs": [],
   "source": [
    "# Provided function to test accuracy\n",
    "# You could check the validation accuracy to select the best of your models\n",
    "def calc_accuracy(preds, tags, padding_id=\"_t_pad_\"):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            preds (np.narray): (num_data, length_sentence)\n",
    "            tags  (np.narray): (num_data, length_sentence)\n",
    "        Output:\n",
    "            Proportion of correct prediction. The padding tokens are filtered out.\n",
    "    \"\"\"\n",
    "    preds_flatten = preds.flatten()\n",
    "    tags_flatten = tags.flatten()\n",
    "    non_padding_idx = np.where(tags_flatten!=padding_id)[0]\n",
    "    \n",
    "    return sum(preds_flatten[non_padding_idx]==tags_flatten[non_padding_idx])/len(non_padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uALnjNRb5Om_",
    "outputId": "eabb6c55-8cac-4b2a-fad6-c5accd048c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Acc: 0.9048193468136191\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred Acc:\", calc_accuracy(val_preds_labels, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "o-vX4nqpUV24"
   },
   "outputs": [],
   "source": [
    "# test set\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'id': test_dict[\"id\"],\n",
    "                   'labels': [json.dumps(np.array(preds).tolist()) for preds in preds_labels]})\n",
    "df.to_csv('test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "I2Z9xp-9UV28",
    "outputId": "d185366e-f546-4c3b-fd96-73780f774857"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[\"O\", \"O\", \"IMMUNE_RESPONSE\", \"IMMUNE_RESPONSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"_t_pad_\", \"_t_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"RESEARCH_ACTIVITY\", \"RESEARCH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"CHEMICAL\", \"CHEMICAL\", \"O\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[\"O\", \"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL\", \"O\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>2945</td>\n",
       "      <td>[\"DATE\", \"O\", \"CORONAVIRUS\", \"O\", \"O\", \"O\", \"O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>2946</td>\n",
       "      <td>[\"VIRUS\", \"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>2947</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"CHEMICAL\", \"O\", \"O\", \"GENE_OR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>2948</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"CHEMICAL\", \"CH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>2949</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             labels\n",
       "0        0  [\"O\", \"O\", \"IMMUNE_RESPONSE\", \"IMMUNE_RESPONSE...\n",
       "1        1  [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"_t_pad_\", \"_t_...\n",
       "2        2  [\"O\", \"O\", \"O\", \"RESEARCH_ACTIVITY\", \"RESEARCH...\n",
       "3        3  [\"O\", \"O\", \"O\", \"CHEMICAL\", \"CHEMICAL\", \"O\", \"...\n",
       "4        4  [\"O\", \"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL\", \"O\",...\n",
       "...    ...                                                ...\n",
       "2945  2945  [\"DATE\", \"O\", \"CORONAVIRUS\", \"O\", \"O\", \"O\", \"O...\n",
       "2946  2946  [\"VIRUS\", \"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL\", ...\n",
       "2947  2947  [\"O\", \"O\", \"O\", \"CHEMICAL\", \"O\", \"O\", \"GENE_OR...\n",
       "2948  2948  [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"CHEMICAL\", \"CH...\n",
       "2949  2949  [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", ...\n",
       "\n",
       "[2950 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DDC10LkU5OnX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lubRmmAW5OnF",
    "jMFljVnI5OnL",
    "Cz70txDd5OnM",
    "Z_6y50zw5OnU"
   ],
   "name": "91-4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
