{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab:  16021\n",
      "Type of train_data_matrix:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Type of test_data_matrix:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of train_data_matrix: (2000, 16021)\n",
      "Shape of test_data_matrix: (400, 16021)\n",
      "Epoch 1/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.4664 - accuracy: 0.4090\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.4377 - accuracy: 0.4410\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.4193 - accuracy: 0.4570\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.4047 - accuracy: 0.4880\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3928 - accuracy: 0.5060: 0s - loss: 0.3762 - accuracy: 0.\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s 12ms/step - loss: 0.3725 - accuracy: 0.5300\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.3646 - accuracy: 0.5575\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.3389 - accuracy: 0.5840\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.3499 - accuracy: 0.5780\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3298 - accuracy: 0.6065\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.3223 - accuracy: 0.6195\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3114 - accuracy: 0.6295\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3137 - accuracy: 0.6415\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3196 - accuracy: 0.6315\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.3171 - accuracy: 0.6445\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.2934 - accuracy: 0.6675\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.2717 - accuracy: 0.7080\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2795 - accuracy: 0.6925\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2708 - accuracy: 0.7045\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.2428 - accuracy: 0.7475\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2353 - accuracy: 0.7395\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3546 - accuracy: 0.5600\n",
      "Training Loss: 0.2352612167596817\n",
      " Training Accuracy: 0.7394999861717224\n",
      "Testng Loss: 0.3545567989349365\n",
      " Testing accuracy: 0.5600000023841858\n"
     ]
    }
   ],
   "source": [
    "#Name: Leung Ko Tsun\n",
    "#SID: 20516287\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "from scipy import sparse\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of documents, a list of labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df['id'], df[\"text\"], df['label']\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)['label']\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "def get_bagofwords(data, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of tokenized documents, type: list\n",
    "    :param vocab_dict: a mapping from words to indices, type: dict\n",
    "    return a BOW matrix in Compressed Sparse Row matrix format, type: scipy.sparse.csr_matrix\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    The BOW matrix is first constructed using Row-based list of lists sparse matrix (LIL) format.\n",
    "    LIL is a convenient format for constructing sparse matrices, as it supports flexible slicing, \n",
    "    and it is efficient to change to the matrix sparsity structure.\n",
    "    '''\n",
    "    \n",
    "    data_matrix = sparse.lil_matrix((len(data), len(vocab_dict)))\n",
    "\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc:\n",
    "            word_idx = vocab_dict.get(word, -1)\n",
    "            if word_idx != -1:\n",
    "                data_matrix[i, word_idx] += 1\n",
    "                \n",
    "    '''\n",
    "    After constructing the BOW matrix on all input documents, we convert the matrix to Compressed Sparse \n",
    "    Row (CSR) format for fast arithmetic and matrix vector operations.\n",
    "    '''\n",
    "    data_matrix = data_matrix.tocsr()\n",
    "    \n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_file = \"data/train.csv\"\n",
    "    test_file = \"data/test.csv\"\n",
    "    ans_file = \"data/answer.csv\"\n",
    "    train_ids, train_texts, train_labels_raw = load_data(train_file)\n",
    "    test_ids, test_texts, _ = load_data(test_file)\n",
    "    test_labels = load_labels(ans_file)\n",
    "\n",
    "    train_data_label = keras.utils.to_categorical(train_labels_raw-1,\n",
    "                                                  num_classes=5)\n",
    "    test_data_label = keras.utils.to_categorical(test_labels-1,\n",
    "                                                 num_classes=5)\n",
    "    # Tokenization\n",
    "    train_tokens = [tokenize(text) for text in train_texts] \n",
    "    test_tokens = [tokenize(text) for text in test_texts]\n",
    "    # Stop words removal\n",
    "    train_tokens = [filter_stopwords(tokens) for tokens in train_tokens]\n",
    "    test_tokens = [filter_stopwords(tokens) for tokens in test_tokens]\n",
    "    # use a set data structure to hold all words appearing in the train set\n",
    "    vocab = set()\n",
    "    for i, doc in enumerate(train_tokens):# enumerate over each document in the train set\n",
    "        # enumerate over each word in the document\n",
    "        for word in doc:\n",
    "            # if this word has been added into the set before, \n",
    "            # then it will be ignored, otherwise, it will be \n",
    "            # added into the set.\n",
    "            vocab.add(word)\n",
    "    # create a dictionary from the set of words, where the\n",
    "    # keys are word strings and the values are numerical indices\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "    print('Size of vocab: ', len(vocab_dict))\n",
    "    train_data_matrix = get_bagofwords(train_tokens, vocab_dict)\n",
    "    test_data_matrix = get_bagofwords(test_tokens, vocab_dict)\n",
    "    print('Type of train_data_matrix: ', type(train_data_matrix))\n",
    "    print('Type of test_data_matrix: ', type(test_data_matrix))\n",
    "    print('Shape of train_data_matrix:', train_data_matrix.shape)\n",
    "    print('Shape of test_data_matrix:', test_data_matrix.shape)\n",
    "    # YOUR CODE HERE\n",
    "    # Data shape\n",
    "    N, V = train_data_matrix.shape\n",
    "    K = train_data_label.shape[1]\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_size = V\n",
    "    output_size = K\n",
    "    batch_size = 100\n",
    "    optimizer = SGD\n",
    "    learning_rate = 0.1\n",
    "    total_epoch = 20\n",
    "\n",
    "    # New model\n",
    "    model = Sequential()\n",
    "\n",
    "    ##### YOUR CODE HERE #######\n",
    "    hidden_size = 100\n",
    "    model.add(Dense(hidden_size, input_shape=(V,)))\n",
    "    model.add(Dense(hidden_size, input_shape=(hidden_size,), activation=\"relu\"))\n",
    "    model.add(Dense(K, input_shape=(hidden_size,), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=SGD(lr=learning_rate), \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "    # training\n",
    "    model.fit(train_data_matrix, train_data_label,\n",
    "              epochs=total_epoch,\n",
    "              batch_size=batch_size)\n",
    "    # testing\n",
    "    train_score = model.evaluate(train_data_matrix, train_data_label,\n",
    "                                 batch_size=batch_size)\n",
    "    test_score = model.evaluate(test_data_matrix, test_data_label,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "          'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "              train_score[0], train_score[1],\n",
    "              test_score[0], test_score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
